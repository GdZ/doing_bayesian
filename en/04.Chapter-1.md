# Chapter 1

## What's in This Book (Read This First!)

### Contents

1.1. Real People Can Read This Book

​	1.1.1. Prerequisites

1.2. What's in This Book

​	1.2.1. You're busy. What's the least you can read?

​	1.2.2. You're really busy! Isn't there even less you can read?

​	1.2.3. You want to enjoy the view a little longer. But not too much longer

​	1.2.4. If you just gotta rejject a null hypothesis

​	1.2.5. Where's the equvalent of traditional test X in this book?

1.3. What's New in the Sencond  Edition?

1.4. Gimme Feedback (Be Polite)

1.5. Thank You!

>  Oh hony I'm searching for love that is true,
>
> But driving throung fog is so dang hard to do.
>
> Please paint me a line on the road to your heart,
>
> I'll rev up my pick up and get a dean start[^1].

### Real People can read this book

This book explains how to actually $do$ Bayesian data analysis, by real people (like you), for realistic data (like yours). The book starts at the basics, with elementary notions of probability and programming. You do not need to already know statistics and programming. The book progresses to advanced hierarchical models that are used in realistic data analysis. This book is speaking to a person such as a first-year graduate student or advanced undergraduate in the social or biological sciences: Someone who grew up in Lake Wobegon[^2], but who is not the mythical being that has the previous training of a nuclear physicist and then decided to learn about Bayesian statistics. (After the publication of the first edition, one of those mythical beings contacted me about the book! So, even if you do have the previous training of a nuclear physicist, I hope the book speaks to you too.)

Details of prerequisites and the contents of the book are presented below. But first things first: As you may have noticed from the beginnning of this chapter, the chapters commence with a stanza of elegant and insightful verse composed by a famous poet. The quatrains are formed of dacylic tetrameter, or, colloquially speaking, "country waltz" meter. The poems regard conceptual themes of the chapter via allusion from immortal human motifs in waltz timimg.

> If you do not find them to be all that funny,
>
> If they leae you wanting back all of your money,
>
> Well, honey, some waltzing's a small price to pay, for
>
> All the good learning you'll get if you stay.

#### 1.1.1. Prerequisites

There is no avoiding mathematics when doing data analysis. On the other hand, this book is definitely not a mathematical statistics textbook, in that it dows not emphasize theorem proving or formal analyses[^6]. But I do expect that you are coming to this book with a dim knowledge of basic calculus. For example, if you understand expressions like $x = \frac{1}{2}x^2$, you're probably good to go. Notice the previous sentence said "understand" the statement of the integral, not "generate" the statement on your own. When matematical derivations are helpful for understanding, they will usually be presented with a thrown blindfolded into the back seat and driven around curves at high speed.

The beginnings of your journey will go more smoothly if you have had some basic experience programming a computer, but previous programming expericence is not crucial. A computer program is just a list of commands that the computer can execute. For example, if you've ever typed an equal sign in an Excel spreadsheet cell, you've written a programming command. If you've ever written a list of commands in Java, C, Python, Basic or any other computer programming language, then you're set. We will be using programming languages called R, JAGS, and Stan, which are free and thoroughly explained in this book.

### 1.2. What's in this book

This book has three major parts. The first part covers foundations: The basic ideas of Bayesian reasoning, models, probabilities, and programming in R.

The second part covers all the crucial ideas of modern Bayesian data analysis while using the simplest possible type of data, namely dichotomous data such as agree/disagree, remember/forget, male/female, etc. Because the data are so simplisitc, the focus can be on Bayesian techniques. In particular, the modern techniques of "Markov chain Monte Carlo" (MCMC) are explained thoroughly and intuitively. Because the data are kept simple in this part of the book, intuitions about the meaning of hierarchical models can be developed in glorious graphic detail. This second part of the book also explores methods for planning how much data will be needed to achieve a desired degree of precision in the conclusions, broadly known as "power analysis".

The third part of the book applies the Bayesian methods to realistic data. The applications are organized around the type of data being analyzed, and the type of measurements that are used to explain or predict the data. Diferent types of measurement scales require different types of mathematical models, but otherwise the underlying concepts are always the same. More details of coverage are provided below.

The chapters of the book are designed to be read in order, for a "grand tour" of basic applied Bayesian analysis. Especially through parts one and two, the chapters probably make the most sense if read in order. But shorter routes are possible, as described next.

#### 1.2.1. You're busy. What's the least you can read?

Here is a minimalist excursion through the book:

- ***Chapter 2***: The idea of Bayesian inferece and model paramters. This chapter introduces important concepts; don't skip it.
- ***Chapter 3***: The R programming language. Read the sections about installing the software, including the extensive set of programm that accompany this book. The rest can be skimmed and returened to later when needed.
- ***Chapter 4***: Basic ideas of probability. Merely skim this chapter if you have high probability of already knowing its content.
- ***Chapter 5***: Bayes rule!
- ***Chapter 6***: The simplest formal application of Bayes rule, referenced throughout the remainder of the book.
- ***Chapter 7***: MCMC methods. This chapter explains the computing method that makes contemporary Bayesian applications possible. You don't need to study all the mathematical details, but you should be sure to get the gist of the pictures.
- ***Chapter 8***: The JAGS programming language for implementing MCMC.
- ***Chapter 16***: Bayesian estimation of two groups. All of the foundational concepts from the aforementioned chapters, applied to the case of comparing two groups.

#### 1.2.2. You're really busy! Isn't there even less you can read?

If all you want is a conceptual foundation and the fastest possible hands-on experience, and if you have some previous knowledge of classical statistics such as a $t$ test, then I recommend the following. First, read ***Chapter 2*** of this book for the conceptual foundation. Then read the article by Kruschke (2013a), which describes Bayesian estimation of two groups (analogous to a traditional $t$ test). Essentially, you've just leapfrogged to ***Chapter 16*** of this book. For your hands-on experience. the article has accompanying software, and there is a version that has been implemented in JavaScript for use in your web browser without need to install other software. For details, see the Web site [http://www.indiana.edu/~kruschke/BEST/](http://www.indiana.edu/~kruschke/BEST/).

### 1.2.3. You want to enjoy the view a little longer. But not too much longer

After the minialist excursion suggested above, if you want to delve into further specific applications, you will need to read these sections:

- ***Chapter 9***: Hierarchical models. Many realistic applications involve hierarchical, or "multilevel" structure. One of the things that makes Bayesian methods so exciting is their seamless applicability to hierarchical models.
- ***Chapter 15***: Overview of the generalized linear model. To know what type of model might apply to your data, you need to know the canonical catalog of conventional models, many of which fall under the umbrella of the generalized linear model.
- Individual chapters from ***Chapters 16-24***. Go to the chapter relevant to the data structure you're interested in (which you'll understand because you previously read ***Chapter 15***).
- ***Chapter 14***: Statistical power analysis and planning of research, from a Bayesian perspective. This chapter is not essential on a first reading, but it's important not to skip forever. After all, failing to plan is planning to fail.
- ***Section 25.1***, which has recommendations for how to report a Bayesian analysis. If you want your research to influence other people, you've got to be able to tell them about it. (Well, I suppose there are other means of persuasion, but you'll have to learn those from other sources.)

#### 1.2.4. If you just gotta reject a null hypothesis...

Traditional statistical methods are often focused on rejecting a null hypothesis, as opposed to estimating magnitudes and their uncetainty. For a Bayesian perspective on null hypotheses, read these chapters:

- ***Chapter 11***: The perils of $p$ values in traditional null-hypothesis significance testing.
- ***Chapter 12***: Bayesian approaches to null value assessment.

#### 1.2.5. Where's the equivalent of traditional test X in this book?

Because many readers will be coming to this book after having already been exposed to tradtional statistics that emphasize null hypothesis significance test (NHST), this book provides Bayesian approaches to many of the usual topics in NHST textbooks. $Table\ 1.1$ lists various test covered by standard introductory statistics textbooks, along with the location of their Bayesian analogus in this book.

The array of tests mentioned in $Table\ 1.1$ are all cases of what is called the "generalized linear model". For those of you already familiar with that term, you can glance ahead to Table 15.3, p. 444, to see which chapters cover which cases. For those of you not yet familiar with that term, do not worry, because all of Chapter 15 is devoted to introducing and explaining the ideas.

A superficial conclusion from Table 1.1 might be, "Gee, the table shows that traditional statistical tests do something analogous to Bayesian analysis." Such a conclusion would be wrong. First, traditional NHST has deep problems, some of which are discussed in Chapter 11. Second, Bayesian analysis yields richer and more informative inferences that NHST, as will be shown in numerous examples throughout the book.

| Traditional analysis name             | Bayesian analogue  |
| ------------------------------------- | ------------------ |
| Binomial test                         | Chapter 6-9 and 21 |
| $t$ test                              | Chapter 16         |
| Simple linear regression              | Chapter 17         |
| Multiple linear regression            | Chapter 18         |
| One-way ANOVA                         | Chapter 19         |
| Multifactor ANOVA                     | Chapter 20         |
| Logistic regression                   | Chapter 21         |
| Multinomial logistic regression       | Chapter 22         |
| Ordinal regression                    | Chapter 23         |
| Chi-square test (contingency table)   | Chapter 24         |
| Power analysis (sample size planning) | Chapter 13         |

### 1.3. What's new in the second edition?

The basic progresson of topics remains the same as the first edition, but all the details have been changed, from cover to cover. The book and its programs have been completely rewritten. Here are juest a few highlights of the changes:

- There are all new program in JAGS and Stan. The new programs are designed to much easier to use that the scripts in the first edition. In particular, there are now compact high-level scripts that make it easy to run the programs on your own data. This new programming was a major undertaking by itself.
- The introductory Chapter 2, regarding the basic ideas of how Bayesian inferece reallocates credibility across possibilities, is completely rewritten and greadly expanded.
- There are completely new chapters on the programming languages R (Chapter 3), JAGS (Chapter 8), and Stan (Chapter 14). The lengthy new chapter on R includes explanations of data files and structures such as lists and data frames, along with several utility functions. (It also has a new poem that I am particularly pleased with.) The new chapter on JAGS includes explanation of the RunJAGS package which executes JAGS on parallel computer cores. The new chapter on Stan provides a novel explanation of the concepts of Hamiltonian Monte Carlo. The chapter on Stan also explains conceptual differences in program flow between Stan and JAGS.
- Chapter 5 on Bayes' rule is greatly revised, with a new emphasis on how Bayes' rule re-allocates credibility across parameter values from prior to posterior. The material on model comparison has been remoed from all the early chapters and integrated into a compact presentation in Chapter 10.
- What were two separate chapters on the Metropolis algorithm and Gibbs sampling have been consolidated into single chaper on MCMC methods (as Chapter 7).
- There is extensive new material on MCMC convergence diagnostics in Chapter 7 and 8. There are explanations of autocorrelation and effective sample size. There is also exploration of the stability of the estimates of the highest density interval (HDI) limits. New computer programs display the diagnostics, as well.
- Chapter 9.





